\documentclass{proc}
\title{\sf Advanced Algorithms Exam Preparations}
\usepackage{mdframed}

\begin{document}
\maketitle

\tableofcontents

\section{Set Cover (with weights)}

Find the collection such that the union of its sets are equal to all elements in U. In weighted version, every set $S_{i}$ has an associated weight $w_{i} \ge 0$.

\begin{mdframed}
    \textbf{Example:}
    
    U = \{$S_{1}, S_{2}, S_{3}, S_{4}$\} = \{\{1,2\}, \{2\}, \{2,3\}, \{3,4,5\}\}
    
    $S_{1} \cup S_{4} = \{1,2,3,4,5\}$ = All elements of U = Set Cover of U.
\end{mdframed}

\textbf{Goal:} Find set cover C such that $\sum_{S_{i} \in C} w_{i}$ is minimised.

Maintain a set R of all remaining uncovered elements.
\begin{mdframed}
    $\frac{w_{i}}{|S_{i} \cap R|} = $ cost for covering remaining elements in $S_{i}$.
\end{mdframed}

\begin{mdframed}
    \textbf{Algorithm:} (Greedy-Set-Cover)
    
    R = U
    
    While $R \neq \emptyset$
    
        \hspace{2ex} Select $S_{i}$ which minimises $\frac{w_{i}}{|S_{i} \cap R|}$.
        
        \hspace{2ex} Delete all $s \in S_{i}$ from R.
        
    End
    
    Return selected sets
\end{mdframed}

\begin{mdframed}
    \textbf{Example:} (bad instance)
    
    $S_{1} = \{1,2,3,4\}, w_{1} = 1 + \epsilon$ ($\epsilon$ small number $> 0$)
    
    $S_{2} = \{5,6,7,8\}, w_{2} = 1 + \epsilon$
    
    $S_{3} = \{3,4,7,8\}, w_{3} = 1$
    
    $S_{4} = \{2, 6\}, w_{4} = 1$
    
    $S_{5} = \{1\}, w_{5} = 1$
    
    $S_{6} = \{5\}, w_{6} = 1$
    
    \vspace{2ex}
    
    \textit{Optimal solution: $2 + 2\epsilon$ $\{S_{1}, S_{2}\}$.}
    
    \vspace{2ex}
    
    1. Picks $S_{3}$ instead of $S_{1}$ or $S_{2}$ since $\frac{1}{4} < \frac{1 + \epsilon}{4}$.
    
    2. Picks $S_{4}$ instead of $S_{1}$ or $S_{2}$ since $\frac{1}{2} < \frac{1 + \epsilon}{2}$
    
    2. Picks $S_{5}$ or $S_{6}$ instead of $S_{1}$ or $S_{2}$ since $\frac{1}{1} < \frac{1 + \epsilon}{1}$
    
    Finds solution: $1 + 1 + \frac{1}{2} + \frac{1}{4} = 2.75 > 2 + 2\epsilon$.
    
    This example can be expanded in the same way to construct an arbitrarily large instance that will perform just as bad.
\end{mdframed}

\begin{mdframed}
    \textbf{Record cost:}

    $c_{s} := \frac{w_{i}}{S_{i} \cap R}$ for every $s \in S_{i} \cap R$
    
    \textit{Does not change the algorithm, used only for analyse.}
\end{mdframed}

\begin{mdframed}
    \textbf{Example:}
    
    $S_{1} = \{1, 3\}, w_{1} = 1$
    
    $S_{2} = \{2,3,4\}, w_{1} = 1$
    
    1. Pick $S_{2}$ of cost $\frac{1}{3} \Rightarrow c_{2} = c_{3} = c_{4} = \frac{1}{3}$
    
    2. Pick $S_{1}$ of cost $\frac{1}{1} \Rightarrow c_{1} = 1, (c_{3} = \frac{1}{3}$, unchanged)
\end{mdframed}

The costs completely account for the total weight of the set cover.

\begin{mdframed}
    \textbf{(11.9)} If C is the set cover obtained by the Greedy-Set-Cover, then $\sum_{S_i \in C} W_i = \sum_{s \in U} c_s$.
\end{mdframed}

\begin{mdframed}
    \textbf{Example:} (continuation of previous)

    $\sum_{S_i \in C} W_i = w_{1} + w_{2} = 1 + 1 = 2$
    
    $\sum_{s \in U} c_s = c_1 + c_2 + c_3 + c_4 = \frac{1}{3} + \frac{1}{3} + \frac{1}{3} + 1 = 2$
    
    $\Rightarrow \sum_{S_i \in C} W_i = \sum_{s \in U} c_s$
\end{mdframed}

\textbf{Q:} How much cost can any single set $S_k$ account for, including sets not picked by the algorithm? Find upper bound for the ratio $\frac{\sum_{s \in S_k} c_s}{w_k}$.

The optimum solution must cover the full cost $\sum_{s \in U} c_s$ via the sets it selects so this bound will establish that it needs to use at least a certain amount of weight ( = lower bound, what we want).

\begin{mdframed}
    \textbf{Harmonic function:}
    
    $H(n) = \sum_{i=1}^{n} \frac{1}{i}$
    
    Sum approximates the area under the curve $y = \frac{1}{x}$.
    
    Naturally bounded above by $1 + \int_{1}^{n} \frac{1}{x} dx = 1 + ln(n)$ and
    
    below by $\int_{1}^{n+1} \frac{1}{x} dx = ln(n + 1)$.
    
    Thus $H(n) = \Theta(ln(n))$
\end{mdframed}

\begin{mdframed}
    \textbf{(11.10)} For every set $S_k$, the sum $\sum_{s \in S_k} c_s$ is at most $H(|S_k|)*w_k$.
    
    \textbf{Proof.}
    Simplify notation by assume that the elements of $S_k$ are the first $d = |S_k|$ elements of the set U ($S_k = \{s_1,\ldots, s_d\}$).
    
    \begin{mdframed}
        \textbf{Example:}
    
        $U = \{a,b,c,d,e,f,g\}$
    
        $S_1 = \{a, b\}, d = |S_1| = 2$
    
        $S_2 = \{a,b,c,d,e\}, d = |S_2| = 5$
    \end{mdframed}
    
    Assume that the elements are labeled in the order in which they are assigned a cost $c_{s_j}$ by the greedy algorithm (ties broken arbitrarily). No loss of generality, only involves renaming elements in U.
    
    Consider the iteration when $s_j$ is covered by the greedy algorithm for some $j \le d$. When the iteration begins, $s_j, s_{j+1}, \ldots, s_d \in R$, according to our naming convention (elements have not been selected). This implies that $|S_k \cap R| \ge d - j + 1$ (there are $d - j + 1$ not already selected elements in $S_k$). The average cost of the set $S_k$ is at most $\frac{w_k}{|S_k \cap R|} \leq \frac{w_k}{d-j+1}$.
    
    It is not necessarily an equality because in the same iteration as $s_j$ is covered by the greedy algorithm, some other elements $s_{j'}$ for $j' < j$ may be covered as well.
    
    In this iteration, the greedy algorithm selects a set $S_i$ of a minimum average cost so that the set $S_i$ has an average cost at most that of $S_k$. The average cost of $S_i$ gets assigned to $s_j$, so
    
    $c_{s_j} = \frac{w_i}{|S_i \cap R|} \le \frac{w_k}{|S_k \cap R|} \le \frac{w_k}{d-j+1}$.
    
    Add up all inequalities for all elements $s \in S_k$:
    
    $\sum_{s \in S_k} c_s = \sum_{j=1}^{d} c_{s_j} \le \sum_{j=1}^{d} \frac{w_k}{d-j+1} = \frac{w_k}{d} + \frac{w_k}{d-1} + \ldots + \frac{w_k}{1} = H(d) * w_k$
\end{mdframed}

Let $d* = max_i |S_i|$ denote the maximum size of any set.

\begin{mdframed}
    \textbf{(11.11)} The set cover C selected by the Greedy-Set-Cover has weight at most H(d*) times the optimal weight w*.
    
    \textbf{Proof.} Let C* denote the optimum weight set cover, so that $w* = \sum_{S_i \in C*} w_i$. For each of the sets in C*, (11.10) implies
    
    $w_i \ge \frac{1}{H(d*)} \sum_{s \in S_i} c_s$
    
    (The cost has to be paid by the weight). Because these sets form a set cover, we have
    
    $\sum_{S_i \in C*} \sum_{s \in S_i} c_s \ge \sum_{s \in U} c_s$
    
    (The same element can be present several times in C* but not in U, there for no equality). Combining these with (11.9) gives the desired bound:
    
    $w* = \sum_{S_i \in C*} w_i \ge \sum_{S_i \in C*} \frac{1}{H(d*)} \sum_{s \in S_i} c_s \ge \frac{1}{H(d*)} \sum_{s \in U} c_s = \frac{1}{H(d*)} \sum_{S_i \in C} w_i$
\end{mdframed}

The greedy algorithm finds a solution within a factor $O(log(d*)$ of the optimal. Since the maximum set size $d*$ can be a constant fraction of the total number of elements n, this is a worst-case upper bound of $O(log(n))$. By expressing the bounds in terms of $d*$ shows us that we're doing much better if the largest set is small.

It has been shown that no polynomial-time approximation algorithm can achieve an approximation bound much better than $H(n)$, unless P = NP. 

\section{Vertex Cover - The Pricing Method}

Vertex cover in a graph $G = (V,E)$ is a set $S \subseteq V$ so that each edge has at least one end in S. In this version of the problem, each vertex $i \in V$ has a weight $w_i \ge 0$, with the weight of a set $S$ of vertices denoted $w(S) = \sum_{i \in S} w_i$. 

\begin{mdframed}
    \textbf{Goal:} find a vertex cover S for which w(S) is minimised.
\end{mdframed}

(When all weights are equal to 1, deciding if there is a vertex cover of weight at most $k$ is the standard decision version of Vertex Cover).

\begin{mdframed}
    Vertex Cover $\le_{P}$ Set Cover
\end{mdframed}

If we had a polynomial-time algorithm that solves the Set Cover Proble, then we could use this algorithm to solve the Vertex Cover Problem in polynomial time.

\begin{mdframed}
    \textbf{(11.12)} The Set Cover approximation algorithm can be used to give an $H(d)$-approximation algorithm for the weight Vertex Cover Problem, where d is the maximum degree of the graph. (The degree of the graph is the maximum number edges attached to a vertex).
    
    \textbf{Proof.} Proof is based on the reduction Vertex Cover $\leq_P $ Set Cover, which also extends to the weighted case. Consider an instance of the weighted Vertex Cover Problem, specified by a graph $G = (V,E)$. We define an instance of Set Cover as follows: the underlying set U is equal to E. For each node $i$, we define a set $S_i$ cosisting of all edges incident to node $i$ and give this set weight $w_i$. Collections of sets that cover U now correspond precisely to vertex cover. Note that the maximum size of any $S_i$ is precisely the maximum degree $d$.
    
    We can therefore use the approximation algorithm for Set Cover to find a vertex cover whose weight is within a factor of $H(d)$ of minimum.
\end{mdframed}

The $H(d)$ approximation is good when $d$ is small but it gets worse as $d$ gets larger, approaching a bound that is logarithmic in the number of vertices.

\begin{mdframed}
    It is not the case that every polynomial-time reduction leads to a comparable implication for approximation algorithms. It is proved that Independent Set $\leq_P$ Vertex Cover but we can't use an approximation algorithm for minimum-size vertex to design a comparably good approximation algorithm for the maximum-size independent set. See example p. 619.
\end{mdframed}

\textbf{The Pricing Method to Minimize Cost} is also known as the primal-dual method. Think of the weights on the nodes as costs and each edge as having to pay for its "share" of the cost of the vertex cover we find. The greedy set cover can be seen as a pricing algorithm where $c_s$ is the cost the algorithm paid for covering the element $s$. The key to proving that the algorithm was an $H(d*)$-approximation algorithm was a certain approximate "fairness" property for the cost-shares. (11.10) shows that the elements in a set $S_k$ are charged by at most an $H(|S_k|)$ factor more than the cost of covering them by the set $S_k$.

In this new algorithm, the weight $w_i$ of the vertex $i$ is seen as the cost for using $i$ in the cover. We will think of each edge $e$ as a separate "agent" who is willing to "pay" something to the node that covers it. The algorithm will not only find a vertex cover $S$ but also determine prices $p_e \ge 0$ for each edge $e \in E$ so that if each edge $e \in E$ pays the price $p_e$, this will in total approximately cover the cost of S. The prices $p_e$ are analogues of $c_s$ from the Set Cover Algorithm.

We call prices $p_e$ fair if for each vertex $i$, the edges adjacent to $i$ do not have to pay more than the cost of the vertex: $\sum_{e=(i,j)} p_e \le w_i$. Fair prices provide a lower bound on the cost of any solution.

\begin{mdframed}
    \textbf{(11.13)} For any vertex cover $S*$, and any nonnegative and fair prices $p_e$, we have $\sum_{e \in E} p_e \le w(S*)$.
    
    \textbf{Proof.} Consider a vertex cover S*. By the definition of fairness, we have $\sum_{e=(i,j)} p_e \le w_i$ for all nodes $i \in S*$.Adding these inequalitities over all nodes in S*, we get
    
    $\sum_{i \in S*} \sum_{e=(i,j)} p_e \le \sum_{i \in S*} w_i = w(S*)$.
    
    Since $S*$ is a vertex cover, each edge $e$ conributes at least one term $p_e$ to the left-hand side. It may contribute more than one copy of $p_e$ to this sum since it may be covered from both ends by $S*$; but the prices are nonnegative and so the sum on the left-hand side is at least as large as the sum of all prices $p_e$:

    $\sum_{e \in E} p_e \le \sum_{i \in S*} \sum_{e=(i,j)} p_e$.
    
    \noindent Combining this with previous inequality:
    
    $\sum_{e \in E} p_e \le w(S*)$.
\end{mdframed}

A node $i$ is \textit{tight} (or "paid for") if $\sum_{e=(i,j)} p_e = w_i$.

\begin{mdframed}
    \textbf{Algorithm:} (Vertex-Cover-Approximation)
    
    Set $p_e = 0$ for all $e \in E$
    
    While exists edge $(i,j)$, neither $i$ nor $j$ is tight

    \hspace{2ex} Select edge $e = (i,j)$

    \hspace{2ex} Increase $p_e$ without violating fairness
    
    EndWhile
    
    Return $S$ = set of all tight nodes
\end{mdframed}

\begin{mdframed}
    \textbf{(11.14)} The set $S$ and the prices $p$ resturned by the algorithm satisfy the inequality $w(S) \le 2 \sum_{e \in E} p_e$.
    
    \textbf{Proof.} All nodes in $S$ are tight, so we have $\sum_{e=(i,j)} p_e = w_i$ for all $i \in S$. Adding over all nodes in $S$ we get
    
    $w(S) = \sum_{i \in S} w_i = \sum_{i \in S} \sum_{e=(i,j)} p_e$.
    
    An edge $e=(i,j)$ can be included in the sum on the right-hand side at most twice (if both $i$ and $j$ are in $S$), and so we get
    
    $w(s) = \sum_{i \in S} \sum_{e=(i,j)} p_e \le 2 \sum_{e \in E} p_e$.
\end{mdframed}

\begin{mdframed}
    \textbf{(11.15)} The set $S$ returned by the algorithm is a vertex cover, and its cost is at most twice the minimum cost of any vertex cover.
    
    \textbf{Proof.} S is indeed a vertex cover. If $S$ would not cover the edge $e=(i,j)$, neither $i$ nor $j$ would be tight and so the while loop should not have ended.
    
    Let $p$ be the prices set by the algorithm and let $S*$ be an optimal vertex cover. By (11.14) we have $2 \sum_{e \in E} p_e \ge w(S)$, and by (11.13) we have $\sum_{e \in E} p_e \le w(S*)$.
    
    The sum of the edge prices is a lower bound on the weight of any vertex cover, and twice the sum of the edge prices is an upper bound on the weight of our vertex cover:
    
    $w(s) \le 2 \sum_{e \in E} p_e \le 2w(S*)$.
\end{mdframed}

\section{The Knapsack Problem - PTAS}
Polynomial-time algorithm has very strong approximation. Consider a more general version of the Knapsack (or Subset Sum). 

\begin{mdframed}
    Suppose you have $n$ items that you consider packing in a knapsack. Each item $i = 1, \ldots, n$ has two integer parameters, a weight $w_i$ and a value $v_i$. Given a knapsack capacity $W$, the goal of the Knapsack Problem is to finda subset $S$ of items of maximum value subject to the restriction that the total weight of the set should not exceed $W$. In other words, maximize $\sum_{i \in S} v_i$ subject to the condition $\sum_{i \in S} w_i \le W$.
    
    Extra parameter $\epsilon$ which is the desired precision. The approximation will find a subset $S$ whose total weight does not exceed $W$, with value $\sum_{i \in S} v_i$ at most a $(1 + \epsilon)$ factor below the maximum possible. 
    The algorithm will run in polyomial time for any fixed choice of $\epsilon > 0$. The dependence on $\epsilon$ will not be polynomial. This type algorithm is called a \textit{polynomial-time approximation scheme}.
\end{mdframed}

The problem with finding a polyonmial time solution is that as the fixed choice of $\epsilon$ get smaller and smaller, the running time gets larger and larger. When $\epsilon$ is small enough to make sure we get the optimum value, it is no longer a polynomial-time algorithm.

In the special case where $v_i = w_i$, there is a dynamic programming algorithm which runs in $O(nW)$. Other variations such as where $v* = max_i v_i$ which has runnning time $O(n^2 v*)$ (only pseudo-code polynomial). Since NP-complete, no polynomial-time algorithm can be found.

Algorithms that depend on the values in a pseudo-polynomial way can often be used to design polynomial-time approximation schemes. This uses the dynamic programming with running time $O(n^2 v*)$ to design PTAS.

If the values are small integers, then $v*$ is small and the problem can be solved in polynomial time already. On the other hand, if the values are large, we do not have to deal with them exactly since we only want an approximate solution. 

\begin{mdframed}
    Define a rounding parameter $b$ and consider the values rounded to an integer multiple of $b$. Use the dynamic programming algorithm to solve the problem with the rounded values, for each item $i$, let its rounded value $\tilde{v_i} = \lceil \frac{v_i}{b} \rceil b$. (Note that the rounded and the original value are quite close to each other).
\end{mdframed}

\begin{mdframed}
    \textbf{(11.34)} For each item $i$ we have $v_i \le \tilde{v_i} \le \title{v_i} + b$.
\end{mdframed}

The gain of this rounding is that all values are multiples of a common value $b$. So, instead of solving the problem with the rounded values $\tilde{v_i}$, we can change the units by dividing allt values by b and get an equivalent problem.

\begin{mdframed}
    Let $\hat{v_i} = \frac{\tilde{v_i}}{b} = \lceil \frac{v_i}{b} \rceil$ for $i = 1, \ldots, n$.
\end{mdframed}

\begin{mdframed}
    \textbf{(11.35)} The Knapsack Problem with values $\tilde{v_i}$ and the scaled problem with values $\hat{v_i}$ have the same set of optimum solutions, the optimum values diff exactly by a factor of $b$, and the scaled values are integral.
\end{mdframed}

Assume all weights $w_i < W$ or they could be deleted directly anyway. For simplicity, assume $\epsilon^{-1}$ is an integer.

\begin{mdframed}
    \textbf{Algorithm:} (Knapsack-Approximation)

    Set $b = \frac{\epsilon}{2n} \max_i v_i$
    
    Solve the Knapsack Problem with values $\hat{v_i}$
    
    Return the set $S$ of items found.
\end{mdframed}

We have only rounded the values and not the weights.

\begin{mdframed}
    \textbf{(11.36)} The set of items $S$ returned by the algorithm has total weight at most $W$, that is $\sum_{i \in S} w_i \le W$.
\end{mdframed}

\begin{mdframed}
    \textbf{(11.37)} The algorithm Knapsack-Approx runs in polynomial time for any fixed $\epsilon > 0$.
    
    \textbf{Proof.} Setting b and rounding all values can be done in polynomial time. The dynamic programming algorithm we use runs in time $O(n^2 v*)$, where $v* = \max_i v_i$. Each item now has weight $w_i$ and value $\hat{v_i}$. To determine the running time, we need to determine $\max_i \hat{v_i}$. The item $j$ with maximum value $v_j = \max_i v_i$ also has maximum value in the rounded problem, so $\max_i \hat{v_i} = \hat{v_j} = \lceil \frac{v_j}{b} \rceil = n \epsilon^{-1}$ (see algorithm when defining b). Hence, the overall running time of the algorithm is $O(n^3 \epsilon^{-1})$. This is polynomial time for any fixed $\epsilon > 0$ as claimed, but the dependence on the desired precision $\epsilon$ is not polynomial, as the running time includes $\epsilon^{-1}$ rather than $\log \epsilon^{-1}$.
\end{mdframed}

\begin{mdframed}
    \textbf{(11.38)} If $S$ is the solution found by the Knapsack-Approximation algorithm, and $S*$ is any other solution satisfying $\sum_{i \in S} w_i \le W$, then we have $(1 + \epsilon) \sum_{i \in S} v_i \ge \sum_{i \in S*} v_i$.
    
    \textbf{Proof.} Let $S*$ be any set satisfying $\sum_{i \in S*} w_i \le W$. Our algorithm finds the optimal solution with values $\tilde{v_i}$, so we know that
    
    $\sum_{i \in S} \tilde{v_i} \ge \sum_{i \in S*} \tilde{v_i}$.
    
    The rounded values $\tilde{v_i}$ and the real values $v_i$ are quite close by (11.34), so we get the following chain of inequalities.
    
    $\sum_{i \in S*} v_i \le \sum_{i \in S*} \tilde{v_i} \le \sum_{i \in S} \tilde{v_i} \le \sum_{i \in S} (v_i + b) \le nb + \sum_{i \in S} v_i $
    
    showing that the value $\sum_{i \in S} v_i$ of the solution we obtain is at most $nb$ smaller than the maximum value possible. We want to obtain a relative error showing that the value obtained, $\sum_{i \in S} v_i$, is at most $(1 + \epsilon)$ factor less than the maximum possible, so we need to compare nb to the value $\sum_{i \in S} v_i$.
    Let $j$ be the item with the largest value. By the choise of $b$, we have $v_j = 2\epsilon^{-1}nb$ and $v_j = \tilde{v_j}$. Since each item alone fits the knapsack, we have $\sum_{i \in S} \tilde{v_i} \ge \tilde{v_j} = 2\epsilon^{-1}nb$. Finally, the chain of inequalities above says $\sum_{i \in S} v_i \ge \sum_{i \in S} \tilde{v_i} - nb$, and thus $\sum_{i \in S} v_i \ge (2\epsilon^{-1} - 1)nb$. Hence $nb \le \epsilon \sum_{i \in S} v_i$ for $\epsilon \le 1$ and so
    
    $\sum_{i \in S*} v_i \le \sum_{i \in S} v_i + nb \le (1 + \epsilon) \sum_{i \in S} v_i$.
\end{mdframed}

For entire algorithm code, see p. 648.

\section{MAX 3-SAT Random Approximation}

Given a set of clauses $C_1, \ldots, C_k$, each of length 3, over a set of variables $X = {x_1, \ldots, x_n}$, does there exist a satisfying truth assignment?

\begin{mdframed}
    $(x_1 \lor x_2 \lor \overline{x_4}) \land (x_1 \lor \overline{x_2} \lor x_4) \land \ldots$
\end{mdframed}


\textbf{Maximum 3-Satisfiability problem, MAX 3-SAT}, find a truth assignment that satisfies as many clauses as possible. This is NP hard since it's NP hard to decide whether the maximum number of simultaneously satisfiable classes is equal to $k$.

Suppose we set each variable $x_1, \ldots, x_n$ independently to 0 or 1 with probability $\frac{1}{2}$. Let $Z$ denote the random variable equal to the number of satisfied clauses. If we decompose $Z$ into a sum of random variables that each take the value 0 or 1; specifically, let $Z_i = 1$ if the clause $C_i$ is satisfied, and 0 otherwise. Thus $Z = Z_1 + Z_2 + \ldots + Z_k$. Now $E[Z_i]$ is equal to the probability that $C_i$ is satisfied, which can be easily computed. In order for $C_i$ to not be satisfied, each of its three variables must be assigned the value that fails to make it true. Since the variables are set independently, the probability of this is $(\frac{1}{2})^3 = \frac{1}{8}$. Thus clause $C_i$ is satisfied with probability $ 1 - \frac{1}{8} = \frac{7}{8}$, and so $E[Z_i] = \frac{7}{8}$. Using linearity of expectation, w see that the expected number of satisfied clauses is $E[Z] = E[Z_1] + E[Z_2] + \ldots + E[Z_k] = \frac{7}{8}k$. Since no assignment can satisfy more than $k$ clauses, we have the following guarantee.

\begin{mdframed}
    \textbf{(13.14)} Consider a 3-SAT formula where each clause has three different variables. The expected number of clauses satisfied by a random assignment is within an approximation factor $\frac{7}{8}$ of optimal.
\end{mdframed}

\begin{mdframed}
    \textbf{(13.15)} For every instance of 3-SAT, there is a truth assigment that satisfies at least a $\frac{7}{8}$ fraction of all clauses.
\end{mdframed}

Every instance of 3-SAT with at most seven clauses is satisfiable. If the instance has $k \le 7$ clauses, then (13.15) implies that there is an assignment satisfying at least $\frac{7}{8}k$ of them. But when $7 \le k$, it follows that $\frac{7}{8}k > k - 1$, and since the number of clauses satisfied by this assignment must be an integer, it must be equal to k. In other words, all clauses are satisfied.

If we can show that the probability a random assignment satisfies at least $\frac{7}{8}k$ clauses is at least $p$, then the expected number of trials performed by the algorithm is $\frac{1}{p}$. We want to show that $p$ is at least as large as an inverse polynomial in $n$ and $k$.

For $j = 0, 1, 2, \ldots, k$, let $p_j$ denote the probability that a random assignment satisfies exactly $j$ clauses.  So the expected number of clauses satisfied, by the definition of expectation, is equal to $\sum_{j=0}^{k} jp_j$, and by the previous analysis, this is equal to $\frac{7}{8}k$. We are interested in the quantity $p = \sum_{j \ge \frac{7}{8}k} p_j$. How can this be used as a lower bound?

We start with 

$\frac{7}{8}k = \sum_{j=0}^{k} jp_j = \sum_{j < \frac{7}{8}k} jp_j + \sum_{j \ge \frac{7}{8}k} jp_j$.

Now let $k'$ denote the largest natural number that is strictly smaller than $\frac{7}{8}k$. The right hand side of the above equation only increases if we replace the terms in the first sum by $k'p_j$, and the terms in the second sum by $k_pj$. We also observe that $\sum_{j<\frac{7}{8}k} p_j = 1 - p$, and so

$ \frac{7}{8}k \le \sum_{j<\frac{7}{8}k} k'p_j + \sum_{j \ge \frac{7}{8}k} kp_j = k'(1-p) + kp \le k' + kp $

and hence $kp \ge \frac{7}{8}k - k'$. But $\frac{7}{8}k - k' \ge \frac{1}{8}$, since $k'$ is a natural number strictly smaller than $\frac{7}{8}$ times another natural number, and so 

$p \ge \frac{\frac{7}{8}k - k'}{k} \ge \frac{1}{8k}$.

This was our goal, to get a lower bound on $p$, and so by the waiting time bound (13.7), we see that the expected number of trials needed to find the satisfying assignment we want is at most $8k$.

\begin{mdframed}
    \textbf{(13.16)} There is a randomised algorithm with polynomial expected running time that is guaranteed to produce a truth assignment satisfying at least a $\frac{7}{8}$ fraction of all clauses.
\end{mdframed}

\section{Randomized Divide and Conquer: Median-Finding and QuickSort}

\end{document}